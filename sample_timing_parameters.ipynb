{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profile Domain Code\n",
    "\n",
    "Below is the profile domain code I've written in Jupyter notebook form. It's useful to play around/test with, but I wouldn't actually run the code from here because (as I understand it) you'd have to keep the notebook running the entire time the code runs, which is a loooooooong time.\n",
    "\n",
    "Anyway, I won't go into much conceptual detail here, because hopefully we'll cover that when we meet, but rather will try to explain the code as much as I can first. Note that this was written using python3 (since it uses PyMC3, which needs python3). I think bowser defaults to python2, so you might need to ask Nate to set up python3 for you.\n",
    "\n",
    "Anywhoo, to run this, you'll need 3 things:\n",
    "\n",
    "1. A par file. Easy enough.\n",
    "2. A \"metafile\". This is just a text file with each line being the name of a data file you'd like to include in the timing solution. If you want an example, there's one here: `/hyrule/data/users/pgentile/prof_domain/Profile_Domain_Timing/datafiles.txt`\n",
    "3. A \"model file\". This is a file that contains a list of the parameters of the gaussians that have been fit to the profile. You'll have to make this yourself. To do that, take the metafile from step 2 and run the following command:\n",
    "\n",
    "    `python2 /home/pgentile/.local/lib/python2.7/site-packages/ppgauss.py -M metafile_you_made.txt`\n",
    "    \n",
    "    That'll bring up a display with a pulse profile, and will print out some instructions on how to fit gaussians to the profile. Follow them and get a \"good enough\" fit. Then press \"p\" to print out that fit. The text that gets spat out can be copied and pasted into a text file.\n",
    "    \n",
    "    Note that this code (the ppgauss one) needs to be run in python2. I know this is super annoying, and it's pretty much my fault, but it's a huge pain to fix, and, well, to be honest it was more important to work on other parts of the code first. Sorry :(.\n",
    "\n",
    "Once you have those files, you should be set to start running the code. \n",
    "\n",
    "For this first block, I'm just importing the tons of modules to be used. Not much insight to be gleaned here, but you need to have installed them all. If you don't have something (like let's say for example you probably don't have pymc3), you can istall it by doing a `pip install pymc3 --user`. If something breaks, Nate should be able to help.\n",
    "\n",
    "One thing to know is that this code assumes the data have been folded with something like `fold_psrfits`.\n",
    "\n",
    "We talked on Friday about Lindley's first paper on this, and I said I'd send it to you. The url is here:\n",
    "\n",
    "https://arxiv.org/pdf/1412.1427.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOK HERE!!!\n",
      " /home/pgentile/.theano/compiledir_Linux-2.6-el6.x86_64-x86_64-with-centos-6.8-Final-x86_64-3.6.3-64\n"
     ]
    }
   ],
   "source": [
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle, time, pint, theano, pypulse, dill\n",
    "import theano.tensor as T\n",
    "import pint.models.model_builder as mb\n",
    "from pint.models.parameter import floatParameter as fp\n",
    "import pint.toa as toa\n",
    "import astropy.units as u\n",
    "from astropy import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, not too much to be gleaned from this next block. The first two lines tell the code how verbose to be. PINT prints errors and warnings for a ton of things through astropy and since they're not important and super annoying, I just shut them off unless it breaks the code (i.e. it's an actual `ERROR`). Theano is the opposite, where if it prints out a warning or an error, it's usually catastrophic, so I've made it very verbose. This still makes the errors it prints out almost completely useless, but it's fun to marvel at.\n",
    "\n",
    "For the purposes of a jupyter notebook, I've also hardcoded the names of the three necessary files listed above, so they're there.\n",
    "\n",
    "Aside from that, it sets you load a trace you've made before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.setLevel(\"ERROR\")\n",
    "theano.config.exception_verbosity='high'\n",
    "load_from_trace = False \n",
    "make_init = True\n",
    "modelfile = \"Profile_Domain_Timing/J1234-3630_model.gmodel\"\n",
    "parfile = \"Profile_Domain_Timing/1235-36.par\"\n",
    "metafile = \"Profile_Domain_Timing/datafiles.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block makes a function to read in the modelfile above, which is just a bunch of gaussians. It also loads in things we don't end up using, like `CODE`and `ALPHA`, but one day we might use them, so hey, why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(pfile):\n",
    "    plines = open(pfile).readlines()\n",
    "    gparams = []\n",
    "    mus = []\n",
    "    sds = []\n",
    "    coeffs = []\n",
    "    for l in plines:\n",
    "        s = l.split()\n",
    "        if l.startswith(\"CODE\"):\n",
    "            code = s[1]\n",
    "        elif l.startswith(\"FREQ\"):\n",
    "            nu_ref = float(s[1])\n",
    "        elif l.startswith(\"DC\"):\n",
    "            DC = float(s[1])\n",
    "        elif l.startswith(\"TAU\"):\n",
    "            tau = float(s[1])\n",
    "        elif l.startswith(\"ALPHA\"):\n",
    "            alpha = float(s[1])\n",
    "        elif l.startswith(\"COMP\"):\n",
    "            mus += [float(s[1])+0.3] \n",
    "            sds += [float(s[5])/(2 * np.sqrt(2 * np.log(2)))]\n",
    "            coeffs += [float(s[9])]\n",
    "    model = [DC] + [tau] + gparams\n",
    "    mus = np.asarray(mus)\n",
    "    sds = np.asarray(sds)\n",
    "    coeffs = np.asarray(coeffs)\n",
    "    return code, nu_ref, alpha, DC, tau, mus, sds, coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block is a little tricky. It takes in the gaussian parameters from above and makes a function that takes in a phase and spits out the template profile intensity at that phase. It then returns that function. Note, you need to do some funny stuff to deal with components that wrap around from phase 0.9 to 0.1 (for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psr_model(DC, mus, sds, coeffs):\n",
    "    def profile(phases):\n",
    "        intens = np.zeros_like(phases, dtype=np.float32)+DC\n",
    "        '''intens = np.zeros_like(phases)+DC'''\n",
    "        for mu, sd, coeff in zip(mus, sds, coeffs):\n",
    "            first = coeff*np.exp((-((phases+1) - mu)**2)/(2*sd**2))\n",
    "            second = coeff*np.exp((-((phases+1) - (mu+1))**2)/(2*sd**2))\n",
    "            third = coeff*np.exp((-((phases+1) - (mu+2))**2)/(2*sd**2))\n",
    "            intens += (first+second+third).astype(np.float64)\n",
    "        norm_intens = (1.0/intens.max())*intens\n",
    "        return norm_intens\n",
    "    return profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block really just combines the previous two blocks so you can go straight from modelfile to profile function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_profile_model(modelfile):\n",
    "\t[model_DC, model_tau, model_mus, model_sds, model_coeffs] = get_params(modelfile)[3:]\n",
    "\treturn psr_model(model_DC, model_mus, model_sds, model_coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next block uses PyPulse (written by our own Michael Lam) to open up an observation file (that's been folded either by the telesope or with fold_psrfits), grab a bunch of useful information like the start MJD and telescope, and grab the data. It then returns all that stuff. Tbin is the time per phase bin, and the \"weights\" bit just zaps all the channels the user wanted to zap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dfile):\n",
    "\tobservatory = \"ao\"\n",
    "\tar = pypulse.Archive(dfile)\n",
    "\tar.dededisperse()\n",
    "\tweights = ar.getWeights()\n",
    "\tdata = ar.getData()[weights>0,:]\n",
    "\tnbin = ar.getNbin()\n",
    "\ttbin = ar.getTbin()\n",
    "\tfreqs = ar.freq.squeeze()[weights>0]\n",
    "\tmjd = ar.header[\"STT_IMJD\"] + (ar.header[\"STT_SMJD\"]+ar.header[\"STT_OFFS\"])/86400.0\n",
    "\tdata /= (np.amax(data, axis=1)[:,None])\n",
    "\t\n",
    "\tscope = ar.getTelescope()\n",
    "\tif scope == \"GBT\":\n",
    "\t\tobservatory = \"gb\"\n",
    "\telif scope == \"Arecibo\":\n",
    "\t\tobservatory = \"ao\"\n",
    "\t\n",
    "\treturn data, freqs, nbin, tbin, mjd, observatory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next block just loops through all the data files in your text file full of file names and uses the `get_data` function defined above to get the data in them. It then stores all that into two big arrays and returns them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_data(meta_file_name):\n",
    "\tdfnames = [x.replace(\"\\n\",\"\") for x in  open(meta_file_name).readlines()]\n",
    "\tdata_array = []\n",
    "\tmeta_array = []\n",
    "\tfor f in dfnames:\n",
    "\t\tdata, freqs, nbin, tbin, mjd, obs = get_data(f)\n",
    "\t\tdata_array.append(data)\n",
    "\t\tmeta_array.append([freqs, nbin, tbin, mjd, obs])\n",
    "\t\n",
    "\tdata_array = np.asarray(data_array)\n",
    "\tmeta_array = np.asarray(meta_array)\n",
    "\treturn data_array, meta_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honestly I don't remember why, but I really wanted to make the timing model have a spin period (in addition to spin frequency, if it had that instead). So I wrote this to make it so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_period(timing_model):\n",
    "\ttry:\n",
    "\t\tperiod = timing_model.P0.value\n",
    "\texcept AttributeError:\n",
    "\t\tp = fp(name = \"P0\", value = 1.0 / timing_model.F0.value, units = \"s\")\n",
    "\t\ttiming_model.add_param_from_top(p, \"\")\n",
    "\t\tperiod = timing_model.P0.value\n",
    "\t\n",
    "\treturn period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next block takes the PINT timing model pbject and makes a dictionary out of it. It's useful because\n",
    "\n",
    "1. Dictionaries play nice with basically all python code and\n",
    "2. It only keeps the \"unfrozen\" parameters (that is, the ones you're actually fitting for).\n",
    "\n",
    "So with this, if you can make a python variable that you call something like `timing_model` and do `timing_model[\"DM\"]` or `timing_model[\"F0\"]` and it'll give you the DM or spin frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timing_model_pardict(timing_model):\n",
    "\tpars = timing_model.params\n",
    "\tpardict = {}\n",
    "\tfor p in pars:\n",
    "\t\tpar = getattr(timing_model, p)\n",
    "\t\tif not par.frozen:\n",
    "\t\t\tpardict[p] = par\n",
    "\treturn pardict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next block is pretty efficiently written, which means its a little hard to read, but lets say you want to view a pulse profile starting at a phase of 0.23 and coverting a full rotation (so, ending at a phase of 1.23), you need to make phases that go from 0.23 to 1, then wrap back around to 0, and continue to 0.23 again. This block does that.\n",
    "\n",
    "The other important thing it does is take in a time (in the form of a TOA) and calculate the phase at that time. This part is done with the `model.phase(toas).frac.value` part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_phases(freqs, toas, model, nbin, sec_per_bin):\n",
    "\tphs = np.tile(np.linspace(0,1,nbin),  (freqs.shape[0],1))\n",
    "\tstart_phases = np.asarray([x%1 for x in model.phase(toas).frac.value])\n",
    "\tphases = start_phases[:,None] + phs\n",
    "\treturn np.mod(phases,1, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of a TOA as you'd see it in a `.tim` file. It's not just an MJD, right? It also includes the frequency and observatory. All this is doing is taking the MJD of the start of an observation the observatory used to take the data, and the frequency of that data (as found in the `metafile` list that's passed to it), and making that into a PINT `TOA` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_TOAs(metadata):\n",
    "\tTOA_list = []\n",
    "\tfor i in range(metadata.shape[1]):\n",
    "\t\tfreqs, nbin, tbin, mjd, observatory = metadata[i]\n",
    "\t\tnew_toas = [toa.TOA((np.modf(mjd)[1], np.modf(mjd)[0]), obs = observatory, freq = freq) for freq in freqs]\n",
    "\t\tTOA_list.append(toa.get_TOAs_list(new_toas))\n",
    "\t\n",
    "\treturn TOA_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you vary the parameters in the par file over and over again to try to find which parameters best describe the data, samplers will often spend some time trying to figure out the best way to do that before actually going at it. This is called \"initializing the sampler\", and it can really take some time. This code lets you save that initialization so if you want to run the code over again (to take more samples), you can just go straight to taking the samples while retaining the benefit of the initialization you did before.\n",
    "\n",
    "Note that I don't think I actually use this, but it's definitely something that's nice to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_init(init_approx, fname=\"dm_init.pkl\"):\n",
    "\tbij = init_approx.approx.groups[0].bij\n",
    "\tsave_param = {param.name: bij.rmap(param.eval()) for param in init_approx.approx.params}\n",
    "\twith open(fname, \"wb\") as initfile:\n",
    "\t\tpickle.dump(save_param, initfile)\n",
    "\t\n",
    "\tprint(\"saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a way to save the initialization from before, you darned well better have a way to load an initialization you've already saved! This does that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_init(init_fname, from_dict=False):\n",
    "\tif not from_dict:\n",
    "\t\twith open(init_fname, \"rb\") as initfile:\n",
    "\t\t\tsave_param = pickle.load(initfile, encoding=\"latin1\")\n",
    "\telse:\n",
    "\t\tsave_param = {'mu': {'DM': np.array(14.326125), 'phase_offset_interval__': np.array(0.70)}, 'rho': {'DM': np.array(0.5), 'phase_offset_interval__': np.array(0.2)}}\n",
    "\t\t\n",
    "\tinit_approx = pm.ADVI()\n",
    "\tbij = init_approx.approx.groups[0].bij\n",
    "\tfor i,p in enumerate(init_approx.approx.params):\n",
    "\t\tinit_approx.approx.params[i].set_value(bij.map(save_param[p.name]))\n",
    "\t\n",
    "\treturn init_approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the actual log likelihood. All that's going on here is you're taking the pulse intensity as predicted by your template at a given phase, and comparing that to the data at that phase in the same way as what we talked about when we met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglike(t_mod, profile_model, toas, metadata, data, noise, offset):\n",
    "\tfreqs, nbin, sec_per_bin, tr1, tr2 = metadata\n",
    "\tphases = (make_phases(freqs, toas, t_mod, nbin, sec_per_bin)+offset)%1\n",
    "\tlprobs = -0.5*np.log(2*np.pi*noise**2) + np.sum(-(profile_model(phases) - data)**2)/ (2*np.pi*noise**2)\n",
    "\treturn lprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most powerful sampling algorithms (like the one we'll use, called NUTS) require the gradient of the log likelihood WRT (with respect to) the parameters you're varying with the sampler. This lets the sampler answer the question, \"If I wanted to increase the log likelihood, do I need to increase or decrease RA? F0? DM?\" That way it can intelligently vary the parameters instead of just jumping around randomly and wasting a ton of time.\n",
    "\n",
    "Unfortunately that means you'll need to actually calculate the gradient. There's nothing clever going on here, I just wrote down the log likelihood (literally, on a blackboard) and took the derivative of it WRT the timing parameters we'll be sampling. When you do that you'll get some things that need explaining:\n",
    "\n",
    "First, you'll eventually find you'll need to calculate the change in phase WRT the change in the parameters. So basically if you give me some MJD and some timing model, I can tell you the phase at that MJD, but I also want to know how much that phase would change if I increased the DM by 1. Or the binary period. Or whatever. This \"change of phase WRT timing model parameters\" is calculated automatically by PINT, which calls it the \"design matrix\". So all the `t_mod.designmatrix(TOAs)` part is doind is calculating the change in phase WRT the timing parameters\n",
    "\n",
    "Second, you'll eventually find you need to know how much the pulse intensity changes at a given phase WRT timing parameters. It's subtle, but this is different from the last thing we talked about. To see why, think about a pulse profile that just has one really narrow component. Let's say that component spans a phase of 0.5 to 0.10, and outside of that, there's no pulse intensity. Now think about what the pulse intensity looks like at a phase of 0.8. At first, there's no intensity. Now, let's rotate the pulse profile by a phase of 0.5 (a big rotation). So now the component spans phases of 0.55 to 0.6. How has the intensity at a phase of 0.8 changed? *It hasn't.* So for this pulsar, even big changes in phase don't necessarily mean a big change in intensity. \n",
    "\n",
    "I've called this \"pulse intensity change at a given phase WRT timing parameters\" part `P_grad`, which stands for \"Profile Gradient\". I calculate it in a block I'll put right below this one.\n",
    "\n",
    "Other than that, the only things I really do are to make sure the gradient of the log likelihood is in the right shape and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(t_mod, noise, offset, profile_model, metadata, P_gradient, data, TOAs):\n",
    "\tfreqs, nbin, sec_per_bin, tr1, tr2 = metadata\n",
    "\tphases = (make_phases(freqs, TOAs, t_mod, nbin, sec_per_bin)+offset)%1\n",
    "\tdiffs = profile_model(phases) - data\n",
    "\tP_grad = P_gradient(phases)\n",
    "\td_phase_d_psr_pars = -t_mod.designmatrix(TOAs)[0][:,1:]\n",
    "\tones_column = np.ones((d_phase_d_psr_pars.shape[0], d_phase_d_psr_pars.shape[1]))#[:,None]\n",
    "\td_phase_d_pars = np.append(d_phase_d_psr_pars, 1*ones_column, axis=1)\n",
    "\tdata_part = np.sum((diffs/(noise**2))*P_grad, axis=1)\n",
    "\tgrad = np.sum(data_part[:,None] *d_phase_d_pars, axis=0)\n",
    "\treturn grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's where I actualy make `P_grad`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_gradient(DC, mus, sds, coeffs):\n",
    "\tdef profile(phases):\n",
    "\t\tintens = np.zeros_like(phases, dtype=np.float32)+DC\n",
    "\t\t'''intens = np.zeros_like(phases)+DC'''\n",
    "\t\tfor mu, sd, coeff in zip(mus, sds, coeffs):\n",
    "\t\t\tfirst = (coeff*np.exp((-((phases+1) - mu)**2)/(2*sd**2)))*((mu - (phases+1))/(sd**2))\n",
    "\t\t\tsecond = (coeff*np.exp((-((phases+1) - (mu+1))**2)/(2*sd**2)))*(((mu+1) - (phases+1))/(sd**2))\n",
    "\t\t\tthird = (coeff*np.exp((-((phases+1) - (mu+2))**2)/(2*sd**2)))*(((mu+2) - (phases+1))/(sd**2))\n",
    "\t\t\tintens += (first+second+third).astype(np.float64)\n",
    "\t\tnorm_intens = (1.0/intens.max())*intens\n",
    "\t\treturn norm_intens\n",
    "\treturn profile\n",
    "\n",
    "def get_P_gradient(modelfile):\n",
    "\t[model_DC, model_tau, model_mus, model_sds, model_coeffs] = get_params(modelfile)[3:]\n",
    "\treturn P_gradient(model_DC, model_mus, model_sds, model_coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whelp, here we go. This is where the train goes off the rails. The big challenge with this code is that the type of things we're doing are totally unlike what most people do. We're not saying, \"well I've gathered test scores for an entire 200 person class. The distribution looks like a gaussian, so fit a gaussian to it and tell me what the mean and standard deviation is.\" This would be easy: gaussians are coded into essentially every stats module in existence.\n",
    "\n",
    "No, we're saying, \"ok, we have a pulse profile, which could be any crazy shape. And we want to see how it's time of arrival is changed by some parameters that are included in some text file somewhere which could include lots of crazy things, and sample those parameters in an intelligent way. Also, we need to correct for the position of the Earth, Solar system shapiro delay, and barycenter the times of arrival.\" There is no module which has this functionality, so we need to make it ourselves. We have PINT, which can do a lot of the pulsar-y things above (correct for shapiro delay and barycenter the data), and we have PyMC3, which can sample parameters in an intelligent way. But these two things cannot talk to each other. So we need to make them talk to each other.\n",
    "\n",
    "This means a lot of what follows will inherit some of the oddities of the way PyMC3 works. For example, math in PyMC3 is done with a module called `theano`, which is some strange Python and C lovechild. It's super fast, but (for example) it means if you make a class that you want theano to be able to use, it has to have a `perform` function inside it. Also, that fuction needs to take an argument called \"node\". Why? Haven't the foggiest! It's just the way it needs to be done. Sorry about that, but it's unavoidable.\n",
    "\n",
    "Anyway, I'll try my best to explain what all the functions below do the best I can. I'm sure you'll be left wanting, but spare a moment for the guy who actually had to write this code.\n",
    "\n",
    "`__init__` - You'll see a lot of familiar faces here. This will take in the filenames you'll work with and read in the data and timing model, make the profile model, make the pardict, and set up the gradient function.\n",
    "\n",
    "\n",
    "`make_period` - This isn't really used anywhere, but it does the same thing as described above.\n",
    "\n",
    "`get_timing_model_pardict` - This does the same thing as described above.\n",
    "\n",
    "`make_pymc3_model` - Isn't used. I should just delete it.\n",
    "\n",
    "`update_timing_model` - When PyMC3 picks new values for the timing model parameters, we want to tell PINT about it and have PINT make a new timing model with those parameters. Lucky, in terms of syntax, this is easy (thanks, PINT!).\n",
    "\n",
    "`fit_model` - Isn't used. I should just delete it.\n",
    "\n",
    "`perform` - Ok, so this is the function that actually calculates the log likelihood. This is the function that PyMC3 calls when it chooses new parameter values (and passes them using the name `inputs`). So when it does that, we want to actually get those new parameter values, update our timing model, calculate the log likelihood, and return it. Normally, when you want to return something from a Python function, you use `return`, but with theano (and therefore PyMC3), you need to store it as `outputs`. So the `outputs[0][0] = np.array(total_logl)` part is essentially the theano equivalent of `return total_logl`.\n",
    "\n",
    "`old_grad` - I used this for a bit for debugging purposes, but it isn't currently used. I should just delete it.\n",
    "\n",
    "`grad` - As explained above, we need to calculate the gradient of the log likelihood WRT the timing parameters every time new parameters are chosen. This does that (in a necessarily roundabout way). Thankfully (and aggrivatingly), you can actually use `return` here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogLikeWithGrad(T.Op):\n",
    "\titypes = [T.dvector]\n",
    "\totypes = [T.dscalar]\n",
    "\t\n",
    "\tdef __init__(self, loglike, filenames):\n",
    "\t\t#print(\"Initializing LLWG\")\n",
    "\t\tself.loglike = loglike\n",
    "\t\tself.dfile, self.parfile, self.modelfile = filenames\n",
    "\t\tself.ofile = open(\"dm_samples.txt\", \"w\")\n",
    "\t\t#self.data, self.freqs, self.nbin, self.sec_per_bin, self.mjdref = get_data(dfile)\n",
    "\t\tself.data, self.metadata = get_all_data(self.dfile)\n",
    "\t\tself.profile_model = get_profile_model(modelfile)\n",
    "\t\tself.P_gradient = get_P_gradient(modelfile)\n",
    "\t\tself.t_mod = mb.get_model(parfile)\n",
    "\t\tself.period = get_period(self.t_mod)\n",
    "\t\tself.init_pepoch = self.t_mod.POSEPOCH.value\n",
    "\t\tself.init_DM = self.t_mod.DM.value\n",
    "\t\tself.pardict = self.get_timing_model_pardict()\n",
    "\t\tself.TOAs = make_TOAs(self.metadata)\n",
    "\t\tself.logpgrad = LogLikeGrad(self.data, self.metadata, self.TOAs, self.profile_model, self.P_gradient, self.pardict, self.t_mod)\n",
    "\t\t#print(\"Initialized\")\n",
    "\t\n",
    "\tdef make_period(self):\n",
    "\t\ttry:\n",
    "\t\t\tperiod = self.t_mod.P0.value\n",
    "\t\texcept AttributeError:\n",
    "\t\t\tp = fp(name = \"P0\", value = 1.0 / self.t_mod.F0.value, units = \"s\")\n",
    "\t\t\tself.t_mod.add_param_from_top(p, \"\")\n",
    "\t\n",
    "\tdef get_timing_model_pardict(self):\n",
    "\t\tpars = self.t_mod.params\n",
    "\t\tpardict = {}\n",
    "\t\tfor p in pars:\n",
    "\t\t\tpar = getattr(self.t_mod, p)\n",
    "\t\t\tif not par.frozen:\n",
    "\t\t\t\tpardict[p] = par\n",
    "\t\treturn pardict\n",
    "\t\n",
    "\tdef make_pymc3_model(self):\n",
    "\t\tbasic_model = pm.Model()\n",
    "\t\tbasic_model_pars = []\n",
    "\t\twith basic_model:\n",
    "\t\t\tfor p in self.pardict.keys():\n",
    "\t\t\t\tpar = self.pardict[p]\n",
    "\t\t\t\tprint(type(par.value), type(par.uncertainty_value))\n",
    "\t\t\t\tvars()[p] = pm.Normal(par.name, mu=float(par.value), sd=float(par.uncertainty_value))\n",
    "\t\t\t\tbasic_model_pars.append(vars()[p])\n",
    "\t\t\tnoise = pm.Bound(pm.Normal, lower=0)(\"noise\", mu=0.66, sd = 0.5)\n",
    "\t\t\tbasic_model_pars.append(noise)\n",
    "\t\t\ttheta = T.as_tensor_variable(basic_model_pars)\n",
    "\t\t\tY_obs = pm.DensityDist(\"Y_obs\", lambda v: self.perform(v), observed={'v': theta})\n",
    "\t\t\n",
    "\t\treturn basic_model\n",
    "\t\n",
    "\tdef update_timing_model(self, theta):\n",
    "\t\tfor key, theta_par in zip(self.pardict.keys(), theta):\n",
    "\t\t\t#print(dir(theta_par))\n",
    "\t\t\t#print(type(theta_par))\n",
    "\t\t\tself.pardict[key].value = theta_par\n",
    "\t\n",
    "\tdef fit_model(self, nits=5000, ntune = 3000, init = \"advi\"):\n",
    "\t\twith self.pymc3_model:\n",
    "\t\t\tself.trace = pm.sample(5000, tune=3000, chains=4, init = \"advi\")\n",
    "\t\n",
    "\tdef perform(self, node, inputs, outputs):\n",
    "\t\t#print(\"Performing!\")\n",
    "\t\t#print(inputs[0])\n",
    "\t\ttheta, = inputs\n",
    "\t\t#print(theta[:-2])\n",
    "\t\t#print(theta[-1])\n",
    "\t\t#print(theta[-2])\n",
    "\t\t#self.ofile.write(\" \".join([str(x) for x in theta]) + \"\\n\")\n",
    "\t\tself.update_timing_model(theta[:-1])\n",
    "\t\t#logl = self.loglike(self.t_mod, self.profile_model, self.freqs, self.data, theta[-2], theta[-1])\n",
    "\t\t#print(self.sec_per_bin)\n",
    "\t\ttotal_logl = 0\n",
    "\t\tfor d, m, ts in zip(self.data, self.metadata, self.TOAs):\n",
    "\t\t\tlogl = self.loglike(self.t_mod, self.profile_model, ts, m, d, 4.0, theta[-1])\n",
    "\t\t\ttotal_logl = total_logl + logl\n",
    "\t\tprint(\"\\nperform: \" + \" \".join([str(x) for x in theta] + [str(logl)]))\n",
    "\t\t\n",
    "\t\toutputs[0][0] = np.array(total_logl)\n",
    "\t\n",
    "\tdef old_grad(self, inputs, g):\n",
    "\t\t'''\n",
    "\t\tthe method that calculates the gradients - it actually returns the\n",
    "        vector-Jacobian product - g[0] is a vector of parameter values\n",
    "\t\t'''\n",
    "\t\t#print(\"Old Grad-ing!\")\n",
    "\t\ttheta = inputs[0].eval()\n",
    "\t\tself.update_timing_model(theta[:-1])\n",
    "\t\tgrads = gradient(self.t_mod, theta[-1], self.profile_model, self.data, self.freqs,\n",
    "\t\t\t\t\t\tself.TOAs)\n",
    "\t\treturn [g[0]*grads]\n",
    "\t\n",
    "\tdef grad(self, inputs, g):\n",
    "\t\ttheta, = inputs\n",
    "\t\tgrads = self.logpgrad(theta)\n",
    "\t\treturn [g[0]*grads]\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because nothing in this world is easy, PyMC3 wants you to make the gradient into it's own class, and because it's a class used by PyMC3, it needs to have a `perform` function, which mysteriously takes in a \"node\" variable. As before, the results aren't returned, but stored as the `outputs` variable. Mercifully, there are only 3 functions for this class:\n",
    "\n",
    "`__init__` - As before, it just sets up some initial variables. Check to see how it's called from `LogLikeWithGrad` if you need more clarity.\n",
    "\n",
    "`update_timing_model` - Because this is a different class than `LogLikeWithGrad`, updating the timing model in `LogLikeWithGrad` won't update the timing model here, so we need to be able to update the timing model here, too.\n",
    "\n",
    "`perform` - Like with `LogLikeWithGrad`, this is the function that actually does the thing. This takes in timing model parameters, calculates the gradient of the log likelihood with respect to them, then stores the result as `outputs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogLikeGrad(T.Op):\n",
    "\t'''\n",
    "\tThis Op will be called with a vector of values and also return a vector of\n",
    "\tvalues - the gradients in each dimension.\n",
    "\t'''\n",
    "\titypes = [T.dvector]\n",
    "\totypes = [T.dvector]\n",
    "    \n",
    "\tdef __init__(self, data, metadata, TOAs, profile_model, P_gradient, pardict, t_mod): #Plus anything else needed to calculate the gradient that doesn't change with new pars\n",
    "\t\tself.data = data\n",
    "\t\tself.metadata = metadata\n",
    "\t\tself.TOAs = TOAs\n",
    "\t\tself.profile_model = profile_model\n",
    "\t\tself.pardict = pardict\n",
    "\t\tself.t_mod = t_mod\n",
    "\t\tself.P_gradient = P_gradient\n",
    "\t\n",
    "\tdef update_timing_model(self, theta):\n",
    "\t\tfor key, theta_par in zip(self.pardict.keys(), theta):\n",
    "\t\t\tself.pardict[key].value = theta_par\n",
    "\t\n",
    "\tdef perform(self, node, inputs, outputs): \n",
    "\t\t'''\n",
    "\t\tThis actually calculates the gradient. Need t_mod and noise. I'm assuming\n",
    "\t\tinputs is whatever we call the functions with in LogLikeWithGrad, rather\n",
    "\t\tthan some other thing PYMC3 just shoves in there. So we can do:\n",
    "\t\t'''\n",
    "\t\t#print(type(inputs[0]))\n",
    "\t\ttheta, = inputs\n",
    "\t\tself.update_timing_model(theta[:-1])\n",
    "\t\t#print(\"Theta shape\", theta.shape)\n",
    "\t\t#print(theta[:-1])\n",
    "\t\t#noise = theta[-2]\n",
    "\t\tnoise = 4.0\n",
    "\t\toffset = theta[-1]\n",
    "\t\ttotal_grads = 0\n",
    "\t\tfor d, m, ts in zip(self.data, self.metadata, self.TOAs):\n",
    "\t\t\tgrads = gradient(self.t_mod, noise, offset, self.profile_model, m, self.P_gradient, d, ts)\n",
    "\t\t\ttotal_grads = total_grads + grads\n",
    "\t\t\n",
    "\t\tprint(total_grads.shape, theta.shape)\n",
    "\t\toutputs[0][0] = total_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're actually doing stuff. We're using the `LogLikeWithGrad` class we made before to set up a likelihood function, then we're defining prior (i.e. initial) distributions of the timing model parameters. The Y_obs is the actualy likelihood being implemented. Then we actually sample! That's the `pm.sample` part. Check out the documentation of that function to see more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llwg = LogLikeWithGrad(loglike, [metafile, parfile, modelfile])\n",
    "basic_model = pm.Model()\n",
    "basic_model_pars = []\n",
    "with basic_model:\n",
    "\tfor p in llwg.pardict.keys():\n",
    "\t\tpar = llwg.pardict[p]\n",
    "\t\tvars()[p] = pm.Normal(par.name, mu=float(par.value), sd=float(10000*par.uncertainty_value), testval = float(par.value))\n",
    "\t\tbasic_model_pars.append(vars()[p])\n",
    "\tphase_offset = pm.Bound(pm.Normal, lower=0, upper=1)(\"phase_offset\", mu=0.46, sd = 0.2, testval=0.46)\n",
    "\tbasic_model_pars.append(phase_offset)\n",
    "\ttheta = T.as_tensor_variable(basic_model_pars)\n",
    "\tY_obs = pm.DensityDist(\"Y_obs\", lambda v: llwg(v), observed={'v': theta})\n",
    "\tstime = time.time()\n",
    "\tprint(\"Began MC stuff at\", time.ctime())\n",
    "\tif not load_from_trace:\n",
    "\t\ttrace = pm.sample(draws = 2000, tune = 1000, chains=1, init = \"advi\")\n",
    "\t\tpm.save_trace(trace)\n",
    "\telse:\n",
    "\t\ttrace = pm.load_trace(tracefile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's really it! Here, we're just plotting and saving a diagnostic plot, then exiting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace)\n",
    "plt.savefig(dfile + \".ps\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"MCMC stuff finished in\", (time.time()-stime)/60.0, \"minutes.\")\n",
    "\n",
    "exit(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that after the above code, I have more stuff, but I don't use it. It's just vestigial code from me trying stuff. You can just delete it if you want and it won't change anything, so I'm not going to bother explaining any of it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
